# -*- coding: utf-8 -*-
"""credit card fraud detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/180UstBDsYaxxvLlW6XDN9iVUIbGWVfze
"""

import pandas as pd
import numpy as np
import tensorflow as tf
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization

card_data = pd.read_csv('/content/creditcard.csv')

card_data.head()

card_data.shape

card_data['Class'].value_counts()

card_data.hist(bins = 30, figsize=(30,30))

card_data.describe()

card_data['Amount'].hist()

"""Values are between 0 - 2500 mainly but there are
outliers, so we will RobustScale the data
"""

#The amount column as very large outliers
# We will do robust scaling to scale the data
card_data['Amount'] = RobustScaler().fit_transform(card_data['Amount'].values.
                                                   reshape(-1,1))

card_data['Amount'].hist()

"""Values are now mostly between 0 and 50"""

card_data['Amount'].describe()

card_data['Time'].hist()

time = card_data['Time']
card_data['Time'] = ((time - time.min())/(time.max() - time.min()))
card_data['Time'].describe()

card_data['Time'].hist()

#Shuffle rows
card_data = card_data.sample(frac=1, random_state=1)

card_data.head()

train, test, val = card_data[:85000], card_data[85000:91000], card_data[91000:]
train['Class'].value_counts(), test['Class'].value_counts(), val['Class'].value_counts()

#Missing values
card_data.isnull().sum()

#Not alot of missing values, so we can drop
card_data.dropna(inplace=True)

train_np, test_np, val_np = train.to_numpy(), test.to_numpy(), val.to_numpy()

print(train.shape, test.shape, val.shape)

x_train, y_train = train_np[:, :-1], train_np[:, -1]
x_test, y_test = test_np[:, :-1], test_np[:, -1]
x_val, y_val = val_np[:, :-1], val_np[:, -1]

print(x_train.shape, y_train.shape)

model = LogisticRegression()
model.fit(x_train, y_train)
model.score(x_train, y_train) #We can use a better accuracy measure

print(classification_report(y_test, model.predict(x_test)))
print(classification_report(y_train, model.predict(x_train)))
print(classification_report(y_val, model.predict(x_val), target_names=['Not fraud',
                                                                                       'Fraud']))

"""Good accuracy but not good precision and recall
precision(not fraud but machine detected fraud)
recall(fraud but machine detected fraud)
"""

#Good accuracy but not good precision and recall
# precision(not fraud but machine detected fraud)
#recall(fraud but machine detected fraud)

#Making own neural model
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import InputLayer, Dense, BatchNormalization
from tensorflow.keras.callbacks import ModelCheckpoint

network = Sequential()
#x_train.shape[0] = rows, x_train.shape[1] = columns(30)
network.add(InputLayer(input_shape=(x_train.shape[1],)))
network.add(Dense(2, 'relu'))
network.add(BatchNormalization())
network.add(Dense(1, 'sigmoid'))
checkpoint = ModelCheckpoint('network.keras', save_best_only=True)
network.compile(optimizer='adam', loss='binary_crossentropy',
                metrics=['accuracy'])

network.summary()

network.fit(x_train, y_train, validation_data=(x_val, y_val),
            epochs=5, callbacks=checkpoint)

def neural_net_predictions(model, x):
  return (model.predict(x).flatten() > .5).astype(int)

print(classification_report(y_val, neural_net_predictions(network, x_val), target_names=['Not Fraud','Fraud']))

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(max_depth = 2, n_jobs = 1)
rf.fit(x_train, y_train)
print(classification_report(y_val, rf.predict(x_val), target_names=['Not fraud','fraud']))

from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0,
                                 max_depth=1, random_state=0)
gbc.fit(x_train, y_train)
print(classification_report(y_val, gbc.predict(x_val), target_names=['Not fraud','fraud']))

from sklearn.svm import LinearSVC
svc = LinearSVC(class_weight='balanced')
svc.fit(x_train, y_train)
print(classification_report(y_val, svc.predict(x_val), target_names=['Not fraud','fraud']))

card_data.head(
)

not_fraud = card_data.query('Class == 0')
fraud = card_data.query('Class == 1')
not_fraud['Class'].value_counts(), fraud['Class'].value_counts()

balanced_dataset = pd.concat([fraud, not_fraud.sample(len(fraud), random_state=1)])
balanced_dataset['Class'].value_counts()

balanced_dataset = balanced_dataset.sample(frac=1, random_state=1)

balanced_dataset

balanced_dataset = balanced_dataset
train_x_b, train_y_b = balanced_dataset[:700, :-1], balanced_dataset[:700, -1]
test_x_b, test_y_b = balanced_dataset[700:842, :-1], balanced_dataset[700:842, -1]
val_x_b, val_y_b = balanced_dataset[842:, :-1], balanced_dataset[842:, -1]

print(train_x_b.shape, test_x_b.shape, val_x_b.shape)

print(train_y_b.shape, test_y_b.shape, val_y_b.shape)

pd.Series(train_y_b).value_counts(), pd.Series(test_y_b).value_counts()

pd.Series(val_y_b).value_counts()

model = LogisticRegression()
model.fit(train_x_b, train_y_b)
model.score(train_x_b, train_y_b)

print(classification_report(val_y_b, model.predict(val_x_b), target_names=['Not fraud',
                                                                                       'Fraud']))

